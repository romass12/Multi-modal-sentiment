{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (4.33.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "import argparse\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from data_prep import batch_iter, createOneHotMosei3way, get_raw_data\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import LSTM_Model\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "unimodal_activations = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal(unimodal_activations, data, classes, attn_fusion=True, enable_attn_2=False, use_raw=True):\n",
    "    \"\"\"\n",
    "    Concatenating three modalities, attention-based lstm network training and choosing the best model\n",
    "    params: \n",
    "    data --> MOSEI is the dataset we are using\n",
    "    classes --> 3 (sentiments)\n",
    "    attn_fusion --> attention based lstm network to reduce the output of Bi-lstm network shape\n",
    "    use_raw --> use raw dataset provided for mosei.\n",
    "    \"\"\"\n",
    "    if use_raw:\n",
    "        if attn_fusion:\n",
    "            attn_fusion = False\n",
    "\n",
    "        train_data, test_data, audio_train, audio_test, text_train, text_test, video_train, video_test, train_label, test_label, seqlen_train, seqlen_test, train_mask, test_mask = get_raw_data(\n",
    "            data, classes)\n",
    "\n",
    "    else:\n",
    "        print(\"starting multimodal\")\n",
    "        text_train = unimodal_activations['text_train']\n",
    "        audio_train = unimodal_activations['audio_train']\n",
    "        video_train = unimodal_activations['video_train']\n",
    "\n",
    "        text_test = unimodal_activations['text_test']\n",
    "        audio_test = unimodal_activations['audio_test']\n",
    "        video_test = unimodal_activations['video_test']\n",
    "\n",
    "        train_mask = unimodal_activations['train_mask']\n",
    "        test_mask = unimodal_activations['test_mask']\n",
    "\n",
    "        print('train_mask', train_mask.shape)\n",
    "\n",
    "        train_label = unimodal_activations['train_label']\n",
    "        print('train_label', train_label.shape)\n",
    "        test_label = unimodal_activations['test_label']\n",
    "        print('test_label', test_label.shape)\n",
    "\n",
    "        seqlen_train = np.sum(train_mask, axis=-1)\n",
    "        print('seqlen_train', seqlen_train.shape)\n",
    "        seqlen_test = np.sum(test_mask, axis=-1)\n",
    "        print('seqlen_test', seqlen_test.shape)\n",
    "\n",
    "    a_dim = audio_train.shape[-1]\n",
    "    v_dim = video_train.shape[-1]\n",
    "    t_dim = text_train.shape[-1]\n",
    "    if attn_fusion:\n",
    "        print('With attention fusion')\n",
    "    allow_soft_placement = True\n",
    "    log_device_placement = False\n",
    "\n",
    "    # Multimodal model\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=allow_soft_placement,\n",
    "        log_device_placement=log_device_placement,\n",
    "        gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "    gpu_device = 0\n",
    "    best_acc = 0\n",
    "    best_loss_accuracy = 0\n",
    "    best_loss = 10000000.0\n",
    "    best_epoch = 0\n",
    "    best_epoch_loss = 0\n",
    "    epochs=5\n",
    "    batch_size = 20\n",
    "    with tf.device('/device:GPU:%d' % gpu_device):\n",
    "        print('Using GPU - ', '/device:GPU:%d' % gpu_device)\n",
    "        with tf.Graph().as_default():\n",
    "            tf.set_random_seed(seed)\n",
    "            sess = tf.Session(config=session_conf)\n",
    "            with sess.as_default():\n",
    "                model = LSTM_Model(text_train.shape[1:], 0.0001, a_dim=a_dim, v_dim=v_dim, t_dim=t_dim,\n",
    "                                   emotions=classes, attn_fusion=attn_fusion,\n",
    "                                   unimodal=False, enable_attn_2=enable_attn_2,\n",
    "                                   seed=seed)\n",
    "                sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))\n",
    "\n",
    "                test_feed_dict = {\n",
    "                    model.t_input: text_test,\n",
    "                    model.a_input: audio_test,\n",
    "                    model.v_input: video_test,\n",
    "                    model.y: test_label,\n",
    "                    model.seq_len: seqlen_test,\n",
    "                    model.mask: test_mask,\n",
    "                    model.lstm_dropout: 0.0,\n",
    "                    model.lstm_inp_dropout: 0.0,\n",
    "                    model.dropout: 0.0,\n",
    "                    model.dropout_lstm_out: 0.0\n",
    "                }\n",
    "\n",
    "                # print('\\n\\nDataset: %s' % (data))\n",
    "                print(\"\\nEvaluation before training:\")\n",
    "                # Evaluation after epoch\n",
    "                step, loss, accuracy = sess.run(\n",
    "                    [model.global_step, model.loss, model.accuracy],\n",
    "                    test_feed_dict)\n",
    "                print(\"EVAL: epoch {}: step {}, loss {:g}, acc {:g}\".format(0, step, loss, accuracy))\n",
    "\n",
    "                for epoch in range(epochs):\n",
    "                    epoch += 1\n",
    "\n",
    "                    batches = batch_iter(list(\n",
    "                        zip(text_train, audio_train, video_train, train_mask, seqlen_train, train_label)),\n",
    "                        batch_size)\n",
    "\n",
    "                    # Training loop. For each batch...\n",
    "                    print('\\nTraining epoch {}'.format(epoch))\n",
    "                    l = []\n",
    "                    a = []\n",
    "                    for i, batch in tqdm(enumerate(batches)):\n",
    "                        b_text_train, b_audio_train, b_video_train, b_train_mask, b_seqlen_train, b_train_label = zip(\n",
    "                            *batch)\n",
    "                        feed_dict = {\n",
    "                            model.t_input: b_text_train,\n",
    "                            model.a_input: b_audio_train,\n",
    "                            model.v_input: b_video_train,\n",
    "                            model.y: b_train_label,\n",
    "                            model.seq_len: b_seqlen_train,\n",
    "                            model.mask: b_train_mask,\n",
    "                            model.lstm_dropout: 0.4,\n",
    "                            model.lstm_inp_dropout: 0.0,\n",
    "                            model.dropout: 0.2,\n",
    "                            model.dropout_lstm_out: 0.2\n",
    "                        }\n",
    "\n",
    "                        _, step, loss, accuracy = sess.run(\n",
    "                            [model.train_op, model.global_step, model.loss, model.accuracy],\n",
    "                            feed_dict)\n",
    "                        l.append(loss)\n",
    "                        a.append(accuracy)\n",
    "\n",
    "                    print(\"\\t \\tEpoch {}:, loss {:g}, accuracy {:g}\".format(epoch, np.average(l), np.average(a)))\n",
    "                    # Evaluation after epoch\n",
    "                    step, loss, accuracy, preds, y, mask = sess.run(\n",
    "                        [model.global_step, model.loss, model.accuracy, model.preds, model.y, model.mask],\n",
    "                        test_feed_dict)\n",
    "                    f1 = f1_score(np.ndarray.flatten(tf.argmax(y, -1, output_type=tf.int32).eval()),\n",
    "                                  np.ndarray.flatten(tf.argmax(preds, -1, output_type=tf.int32).eval()),\n",
    "                                  sample_weight=np.ndarray.flatten(tf.cast(mask, tf.int32).eval()), average=\"weighted\")\n",
    "                    print(\"EVAL: After epoch {}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(epoch, step,\n",
    "                                                                                               loss / test_label.shape[\n",
    "                                                                                                   0],\n",
    "                                                                                               accuracy, f1))\n",
    "                    if accuracy > best_acc:\n",
    "                        best_epoch = epoch\n",
    "                        best_acc = accuracy\n",
    "                    if loss < best_loss:\n",
    "                        best_loss = loss\n",
    "                        best_loss_accuracy = accuracy\n",
    "                        best_epoch_loss = epoch\n",
    "\n",
    "                print(\n",
    "                    \"\\n\\nBest epoch: {}\\nBest test accuracy: {}\\nBest epoch loss: {}\\nBest test accuracy when loss is least: {}\".format(\n",
    "                        best_epoch, best_acc, best_epoch_loss, best_loss_accuracy))\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(sess,\"./dataset/network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape for audio test data (678, 98, 74)\n",
      "Shape for text test data (678, 98, 300)\n",
      "Shape for video test data (678, 98, 35)\n",
      "audio train shape is (2250, 98, 74)\n",
      "audio test shape is (678, 98, 74)\n",
      "Trimodal Train data shape (2250, 98, 409)\n",
      "Trimodal Test data shape (678, 98, 409)\n",
      "Using GPU -  /device:GPU:0\n",
      "Trainable parameters: 314403\n",
      "\n",
      "Evaluation before training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: epoch 0: step 0, loss 0.000379732, acc 1\n",
      "\n",
      "Training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113it [01:22,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t \tEpoch 1:, loss 0.000375799, accuracy 0.431751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: After epoch 1: step 113, loss 5.51662e-07, acc 0.715865, f1 0.834407\n",
      "\n",
      "Training epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113it [00:51,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t \tEpoch 2:, loss 0.000382683, accuracy 0.420226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: After epoch 2: step 226, loss 5.88749e-07, acc 1, f1 1\n",
      "\n",
      "Training epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113it [00:53,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t \tEpoch 3:, loss 0.000431739, accuracy 0.485287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: After epoch 3: step 339, loss 6.98223e-07, acc 1, f1 1\n",
      "\n",
      "Training epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113it [00:51,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t \tEpoch 4:, loss 0.000529975, accuracy 0.510638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: After epoch 4: step 452, loss 8.7748e-07, acc 1, f1 1\n",
      "\n",
      "Training epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113it [00:51,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t \tEpoch 5:, loss 0.000674671, accuracy 0.523098\n",
      "EVAL: After epoch 5: step 565, loss 1.12529e-06, acc 1, f1 1\n",
      "\n",
      "\n",
      "Best epoch: 2\n",
      "Best test accuracy: 1.0\n",
      "Best epoch loss: 1\n",
      "Best test accuracy when loss is least: 0.7158647775650024\n"
     ]
    }
   ],
   "source": [
    "multimodal(unimodal_activations,'mosei',3, True,True, use_raw=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NOW FOR TESTING THE MODEL , LOAD THE NETWORK META DATA AND RUN ON YOUR DATA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sess =  tf.Session() \n",
    "saver = tf.train.import_meta_graph('./dataset/network.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('./dataset'))\n",
    "graph = tf.get_default_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
